{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e2e681-3c09-4dd2-a9c0-81010d96c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mauro.dalleluccatosi/Documents/environments/gradient_push/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7641d-b1b0-441a-88ed-1095686cfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1febbb85-40e7-4688-bf17-2ffd7442d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    transformations = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4802, 0.4481, 0.3975], std=[0.2296, 0.2263, 0.2255])\n",
    "    ])\n",
    "    imgs = [transformations(item['image'].convert('RGB')) for item in batch]\n",
    "    lbs = [item['label'] for item in batch]\n",
    "    return (imgs,lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d27e4fd-8f46-4c21-be95-b97163af74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(batch_size=128, num_workers=7, train=True):\n",
    "    if train:\n",
    "        trainset = load_dataset('Maysee/tiny-imagenet', split='train')\n",
    "        return torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, shuffle_seed=random_seed, collate_fn=collate_fn)\n",
    "    \n",
    "    else:\n",
    "        evalset = load_dataset('Maysee/tiny-imagenet', split='valid')\n",
    "        return torch.utils.data.DataLoader(evalset, batch_size=batch_size, shuffle=True, shuffle_seed=random_seed, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06bbc5c9-1f97-4a28-b5b6-8e84fb6cf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet50_pytorch import ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3539025c-c878-43c2-af5f-f715a666b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = init_model()\n",
    "model = ResNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 5\n",
    "loss_sum = 0\n",
    "print_frequency = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6403c9f-db33-40c2-a3f5-9be3aaf767b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = make_dataloader(train=True)\n",
    "val_loader = make_dataloader(train=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49e25481-9168-43a4-84d5-2e42e80c8746",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffdff5e0-cfda-4213-864c-99e50ebd1630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.7612942968096051\n",
      "0 7 2.65656045565681\n",
      "0 14 3.498048614178385\n",
      "0 21 3.4260009740080153\n",
      "0 28 3.43661061248609\n",
      "0 35 3.162638438067266\n",
      "0 42 2.484109029173851\n",
      "0 49 3.9342805487768993\n",
      "0 56 3.7016399077006747\n",
      "0 63 3.392662295273372\n",
      "0 70 3.3453642640795027\n",
      "0 77 2.9021721865449632\n",
      "0 84 3.9020178339311054\n",
      "0 91 3.6820112679685866\n",
      "0 98 3.609568510736738\n",
      "0 105 2.765452401978629\n",
      "0 112 3.460080095699855\n",
      "0 119 4.320494174957275\n",
      "0 126 4.085704045636313\n",
      "0 133 3.8572702748434886\n",
      "0 140 3.7640933990478516\n",
      "0 147 3.8655047586985996\n",
      "0 154 4.513715539659772\n",
      "0 161 4.148005247116089\n",
      "0 168 4.214187008993966\n",
      "0 175 3.774543251310076\n",
      "0 182 4.3071174791881015\n",
      "0 189 4.586383308683123\n",
      "0 196 4.396456360816956\n",
      "0 203 4.4124767780303955\n",
      "0 210 3.7985069240842546\n",
      "0 217 4.758507285799299\n",
      "0 224 4.545607669012887\n",
      "0 231 4.3895823274339945\n",
      "0 238 4.257874148232596\n",
      "0 245 4.4646658556801935\n",
      "0 252 4.859663384301322\n",
      "0 259 4.612051827566964\n",
      "0 266 4.670415844236102\n",
      "0 273 4.546453237533569\n",
      "0 280 4.556282690593174\n",
      "0 287 4.847173282078335\n",
      "0 294 4.827048710414341\n",
      "0 301 4.656994751521519\n",
      "0 308 4.409238168171474\n",
      "0 315 4.7202677726745605\n",
      "0 322 4.960463012967791\n",
      "0 329 4.924385343279157\n",
      "0 336 4.805587121418545\n",
      "0 343 4.539304665156773\n",
      "0 350 4.876424482890537\n",
      "0 357 4.9079819066183905\n",
      "0 364 4.824442182268415\n",
      "0 371 4.69787859916687\n",
      "0 378 4.658310788018363\n",
      "0 385 5.100880452564785\n",
      "0 392 5.062858070646014\n",
      "0 399 4.8453241756984164\n",
      "0 406 4.75449504171099\n",
      "0 413 4.8380071095057895\n",
      "0 420 5.18168044090271\n",
      "0 427 5.1083027975899835\n",
      "0 434 4.940120765141079\n",
      "0 441 4.951613800866263\n",
      "0 448 4.989770344325474\n",
      "0 455 5.149025406156268\n",
      "0 462 5.096697807312012\n",
      "0 469 5.174199308667864\n",
      "0 476 4.748408385685512\n",
      "0 483 5.173125948224749\n",
      "0 490 5.233933244432722\n",
      "0 497 5.15678037915911\n",
      "0 504 5.140371969767979\n",
      "0 511 4.9713367734636575\n",
      "0 518 5.272574969700405\n",
      "0 525 5.238837582724435\n",
      "0 532 5.122988496507917\n",
      "0 539 5.165752785546439\n",
      "0 546 4.9276705128805975\n",
      "0 553 5.397878783089774\n",
      "0 560 5.388717447008405\n",
      "0 567 5.209376880100796\n",
      "0 574 5.222691740308489\n",
      "0 581 5.052011626107352\n",
      "0 588 5.449623176029751\n",
      "0 595 5.289764676775251\n",
      "0 602 5.2608184814453125\n",
      "0 609 5.177933216094971\n",
      "0 616 5.217048713139126\n",
      "0 623 5.378261498042515\n",
      "0 630 5.318614550999233\n",
      "0 637 5.256850106375558\n",
      "0 644 5.26337514604841\n",
      "0 651 5.322605950491769\n",
      "0 658 5.442894322531564\n",
      "0 665 5.425764969417027\n",
      "0 672 5.346076692853655\n",
      "0 679 5.235390595027378\n",
      "0 686 5.396501200539725\n",
      "0 693 5.422881058284214\n",
      "0 700 5.428725787571499\n",
      "0 707 5.325581618717739\n",
      "0 714 5.20096002306257\n",
      "0 721 5.478801795414516\n",
      "0 728 5.4536647115434915\n",
      "0 735 5.370850835527692\n",
      "0 742 5.240084035055978\n",
      "0 749 5.256904874529157\n",
      "0 756 5.42430179459708\n",
      "0 763 5.474366869245257\n",
      "0 770 5.354274749755859\n",
      "0 777 5.284564222608294\n",
      "epoch_loss 3667.104294695484\n",
      "1 0 3.2160188811165944\n",
      "1 7 5.262147767203195\n",
      "1 14 5.39888858795166\n",
      "1 21 5.365156991141183\n",
      "1 28 5.242901120867048\n",
      "1 35 5.224367822919573\n",
      "1 42 5.239687987736294\n",
      "1 49 5.4411347934177945\n",
      "1 56 5.359624113355364\n",
      "1 63 5.397425174713135\n",
      "1 70 4.990280219486782\n",
      "1 77 5.207938126155308\n",
      "1 84 5.360706465584891\n",
      "1 91 5.099335432052612\n",
      "1 98 5.177330834524972\n",
      "1 105 5.215875489371164\n",
      "1 112 5.1785401957375665\n",
      "1 119 5.362017018454416\n",
      "1 126 5.374416146959577\n",
      "1 133 5.260488986968994\n",
      "1 140 5.27430602482387\n",
      "1 147 5.296847888401577\n",
      "1 154 5.3910128729684015\n",
      "1 161 5.332367352076939\n",
      "1 168 5.2878281729561945\n",
      "1 175 5.064934458051409\n",
      "1 182 5.283063343593052\n",
      "1 189 5.380201271602085\n",
      "1 196 5.19901534489223\n",
      "1 203 5.1435787337166925\n",
      "1 210 5.068283421652658\n",
      "1 217 5.403153691973005\n",
      "1 224 5.319779123578753\n",
      "1 231 5.263917650495257\n",
      "1 238 5.186690875462124\n",
      "1 245 5.2388249805995395\n",
      "1 252 5.401516914367676\n",
      "1 259 5.299595628465925\n",
      "1 266 5.26087086541312\n",
      "1 273 5.196971075875418\n",
      "1 280 5.2095809664045065\n",
      "1 287 5.3959126472473145\n",
      "1 294 5.33058500289917\n",
      "1 301 5.248778002602713\n",
      "1 308 5.202066830226353\n",
      "1 315 5.307800156729562\n",
      "1 322 5.169053588594709\n",
      "1 329 5.1189015592847555\n",
      "1 336 5.180642264229911\n",
      "1 343 5.112349442073277\n",
      "1 350 5.304585252489362\n",
      "1 357 5.366559164864676\n",
      "1 364 5.2371736254010886\n",
      "1 371 5.2544869014195035\n",
      "1 378 5.098965849195208\n",
      "1 385 5.360043457576206\n",
      "1 392 5.3545388494219095\n",
      "1 399 5.129670143127441\n",
      "1 406 5.174699442727225\n",
      "1 413 5.1910617010934015\n",
      "1 420 5.431910719190325\n",
      "1 427 5.334398678370884\n",
      "1 434 5.17988531930106\n",
      "1 441 5.192836761474609\n",
      "1 448 5.239495277404785\n",
      "1 455 5.388967786516462\n",
      "1 462 5.186042615345547\n",
      "1 469 5.231780597141811\n",
      "1 476 4.823866707938058\n",
      "1 483 5.055667127881732\n",
      "1 490 5.295958246503558\n",
      "1 497 5.26285103389195\n",
      "1 504 5.279775210789272\n",
      "1 511 4.979826654706683\n",
      "1 518 5.294974258967808\n",
      "1 525 5.358930587768555\n",
      "1 532 5.218620845249721\n",
      "1 539 5.168793882642474\n",
      "1 546 5.1315933636256625\n",
      "1 553 5.424065998622349\n",
      "1 560 5.427839892251151\n",
      "1 567 5.314297267368862\n",
      "1 574 5.322091920035226\n",
      "1 581 5.1970681462969095\n",
      "1 588 5.4411676951817105\n",
      "1 595 5.362021446228027\n",
      "1 602 5.3039961542402\n",
      "1 609 5.175964287349156\n",
      "1 616 5.306447233472552\n",
      "1 623 5.43121474129813\n",
      "1 630 5.366144725254604\n",
      "1 637 5.322410651615688\n",
      "1 644 5.3178935050964355\n",
      "1 651 5.284961223602295\n",
      "1 658 5.3711404119219095\n",
      "1 665 5.381640706743513\n",
      "1 672 5.330910887037005\n",
      "1 679 5.263148920876639\n",
      "1 686 5.31196015221732\n",
      "1 693 5.3886333874293735\n",
      "1 700 5.3204802104405\n",
      "1 707 5.293718269893101\n",
      "1 714 5.1871941430228095\n",
      "1 721 5.356449059077671\n",
      "1 728 5.332570825304304\n",
      "1 735 5.309429168701172\n",
      "1 742 5.167647055217198\n",
      "1 749 5.046398878097534\n",
      "1 756 5.3359150886535645\n",
      "1 763 5.264235155923026\n",
      "1 770 5.264040470123291\n",
      "1 777 5.259404863630023\n",
      "epoch_loss 4112.98371720314\n",
      "2 0 3.1338208062308177\n",
      "2 7 5.113801547459194\n",
      "2 14 5.322099617549351\n",
      "2 21 5.367449556078229\n",
      "2 28 5.250606536865234\n",
      "2 35 5.310283865247454\n",
      "2 42 5.299608775547573\n",
      "2 49 5.431683199746268\n",
      "2 56 5.345732416425433\n",
      "2 63 5.3029095104762485\n",
      "2 70 5.323924745832171\n",
      "2 77 5.308337143489292\n",
      "2 84 5.413406031472342\n",
      "2 91 5.327856063842773\n",
      "2 98 5.349453721727643\n",
      "2 105 5.290238380432129\n",
      "2 112 5.366863591330392\n",
      "2 119 5.389763014657157\n",
      "2 126 5.394021987915039\n",
      "2 133 5.330070427485874\n",
      "2 140 5.329673903329032\n",
      "2 147 5.360828808375767\n",
      "2 154 5.421465941837856\n",
      "2 161 5.4106550216674805\n",
      "2 168 5.27774395261492\n",
      "2 175 5.314123289925711\n",
      "2 182 5.397214140210833\n",
      "2 189 5.404213019779751\n",
      "2 196 5.332271167210171\n",
      "2 203 5.333409718104771\n",
      "2 210 5.2774699074881415\n",
      "2 217 5.392861434391567\n",
      "2 224 5.351718153272357\n",
      "2 231 5.343814100537982\n",
      "2 238 5.288285323551723\n",
      "2 245 5.340434483119419\n",
      "2 252 5.421299866267613\n",
      "2 259 5.391237463269915\n",
      "2 266 5.321314130510602\n",
      "2 273 5.326713902609689\n",
      "2 280 5.3163725308009555\n",
      "2 287 5.441261904580252\n",
      "2 294 5.375597272600446\n",
      "2 301 5.356691564832415\n",
      "2 308 5.317783287593296\n",
      "2 315 5.331263474055699\n",
      "2 322 5.327649184635708\n",
      "2 329 5.305766718728202\n",
      "2 336 5.35878814969744\n",
      "2 343 5.226578371865409\n",
      "2 350 5.349731309073312\n",
      "2 357 5.330051217760358\n",
      "2 364 5.324264117649624\n",
      "2 371 5.300675460270473\n",
      "2 378 5.253626550946917\n",
      "2 385 5.370411804744175\n",
      "2 392 5.359273774283273\n",
      "2 399 5.315589427947998\n",
      "2 406 5.2929824420384\n",
      "2 413 5.266894953591483\n",
      "2 420 5.39953477042062\n",
      "2 427 5.358229296548026\n",
      "2 434 5.262111050742013\n",
      "2 441 5.224061897822788\n",
      "2 448 5.203317642211914\n",
      "2 455 5.327718530382429\n",
      "2 462 5.25101021357945\n",
      "2 469 5.241713319505964\n",
      "2 476 5.125251906258719\n",
      "2 483 5.214820384979248\n",
      "2 490 5.302621500832694\n",
      "2 497 5.201498372214181\n",
      "2 504 5.044193199702671\n",
      "2 511 5.043669564383371\n",
      "2 518 5.143241269247873\n",
      "2 525 5.1490998949323386\n",
      "2 532 5.040680204119001\n",
      "2 539 4.794804981776646\n",
      "2 546 4.661732026508877\n",
      "2 553 5.091617992946079\n",
      "2 560 4.9864734922136575\n",
      "2 567 4.731290238244193\n",
      "2 574 4.637721947261265\n",
      "2 581 4.569758960178921\n",
      "2 588 5.039632184164865\n",
      "2 595 4.903814281736102\n",
      "2 602 4.771331548690796\n",
      "2 609 4.632206167493548\n",
      "2 616 4.666644845690046\n",
      "2 623 4.934307473046439\n",
      "2 630 4.955208948680332\n",
      "2 637 4.8206703662872314\n",
      "2 644 4.634722948074341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# optimization update\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss_sum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/environments/gradient_push/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/environments/gradient_push/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(True)\n",
    "epoch_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    _train_loader = train_loader.__iter__()\n",
    "    \n",
    "    itr = 0\n",
    "    for i, (batch, target) in enumerate(_train_loader, start=itr):\n",
    "        if(i > len(train_loader)-2):\n",
    "            break\n",
    "\n",
    "        batch = torch.stack(batch, dim=0)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # optimization update\n",
    "        loss_sum+=loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        if i % print_frequency == 0:\n",
    "            print(epoch, i, loss_sum/print_frequency)\n",
    "            loss_sum=0\n",
    "            \n",
    "    print(\"epoch_loss\", epoch_loss)\n",
    "    epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782724f-32a8-4c39-8ede-0caa06ebb387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
