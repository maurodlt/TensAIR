{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1a0b14-17f2-41a7-9ad7-5bc78e32dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8917100d-b96d-4ae4-97f8-2ec12a8f4f80",
   "metadata": {},
   "source": [
    "## Define dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4772bf-a416-4cf0-8f20-93ee637a6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000 # Define the vocabulary size\n",
    "sequence_length = 15 #number of words in a sequence\n",
    "window_size=3 #size of window in skip-gram\n",
    "num_ns=4 #negative samples\n",
    "dataset_name= \"shakespeare\" #or \"wikipedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78146f92-5206-442f-99af-da5c5ce683d8",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d928e65-0db2-494e-a889-9ee797b029ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966f0825-e832-4d97-954d-13250451572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training data\n",
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data_word2vec(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence, \n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples \n",
    "        # with positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1, \n",
    "              num_sampled=num_ns, \n",
    "              unique=True, \n",
    "              range_max=vocab_size, \n",
    "              seed=SEED, \n",
    "              name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context.numpy())\n",
    "            labels.append(label.numpy())\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04150b15-6b5a-4a8e-a968-8d61ddb682f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose between shakespeare and wikipedia dataset\n",
    "def choose_dataset(dataset_name):\n",
    "    \n",
    "    if(dataset_name==\"wikipedia\"):\n",
    "        wikipedia = tfds.load('wikipedia/20201201.en', split='train[:1%]', shuffle_files=True)\n",
    "        assert isinstance(wikipedia, tf.data.Dataset)\n",
    "        tfds.as_numpy(wikipedia)\n",
    "        lines = []\n",
    "        for article in tfds.as_numpy(wikipedia):\n",
    "            lines += article[\"text\"].splitlines()\n",
    "\n",
    "        lines = [l for l in lines if l != \"\".encode()]\n",
    "    else:\n",
    "        path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "        with open(path_to_file) as f: \n",
    "            lines = f.read().splitlines()\n",
    "            \n",
    "    return tf.convert_to_tensor(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2551590-e1aa-4d9d-ad8a-56528e3c8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download dataset\n",
    "lines = choose_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef12344-1824-4241-9008-e35e6d1a6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize sentences from the corpus\n",
    "# We create a custom standardization function to lowercase the text and remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a827773-f243-4906-89a5-7a320d818ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.Dataset.from_tensor_slices(lines).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033c2e97-c0d9-48b0-9e93-360de9fd1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 18:43:13.135463: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a7bba5-ecc7-49d0-81f4-f082a5b07fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce9d194-59fc-4ef1-8135-f641be2964a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return tf.squeeze(vectorize_layer(text))\n",
    "\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83ef12c7-f1ad-4f53-bd74-7b7c9efe5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "611b9118-e265-4b6b-b30c-efe137350c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 32777/32777 [00:05<00:00, 6401.01it/s]\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data_word2vec(\n",
    "    sequences=sequences, \n",
    "    window_size=window_size, \n",
    "    num_ns=num_ns, \n",
    "    vocab_size=vocab_size, \n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "176b6dd3-303c-453d-9d9f-0b367708b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "evaluate_length = int(len(targets) * 0.15)\n",
    "evaluate_dataset = tf.data.Dataset.from_tensor_slices(((targets[:evaluate_length], contexts[:evaluate_length]), labels[:evaluate_length])).shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "evaluate_dataset = evaluate_dataset.batch(evaluate_length, drop_remainder=True)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((targets[evaluate_length:], contexts[evaluate_length:]), labels[evaluate_length:])).shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1772c81-de6c-4437-9c75-ca4f312475db",
   "metadata": {},
   "source": [
    "## Save training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58edbd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensair_path = os.environ.get(\"TENSAIR_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4638a99-18d3-4d0e-8b00-1857cacc8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(tensair_path + \"/data/W2V/\"+dataset_name+\"_train.txt\", \"w\")\n",
    "for d in train_dataset:\n",
    "    (targets, contexts), labels = d\n",
    "    for target, context, label in zip(targets,contexts, labels):\n",
    "        target = target.numpy()\n",
    "        context = context.numpy()\n",
    "        label = label.numpy()\n",
    "        example = \"\" + str(target) + \" \"\n",
    "        example += str(context[0][0]) + \" \" + str(context[1][0]) + \" \" + str(context[2][0]) + \" \" + str(context[3][0]) + \" \" + str(context[4][0]) + \" \"\n",
    "        example += str(label[0]) + \" \" + str(label[1]) + \" \" + str(label[2]) + \" \" + str(label[3]) + \" \" + str(label[4]) + \"\\n\"\n",
    "        f.write(example)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fae6e-990b-445b-80a8-987c280724a8",
   "metadata": {},
   "source": [
    "## Save evaluaion dataset (in byte format acceptted by TensAIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37759de2-1e19-44c5-bfba-6cf60ba3c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [] \n",
    "contexts = []\n",
    "labels = []\n",
    "for d in evaluate_dataset:\n",
    "    (t, c), l = d\n",
    "    target = list(t.numpy())\n",
    "    contexts = list(c.numpy())\n",
    "    labels = list(l.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba6b1ecb-5aab-4e11-8193-0207ba78665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_int = 4 # int = 4 bytes in c++\n",
    "mini_batch_size = 2048\n",
    "with open(tensair_path + \"/data/W2V/\"+dataset_name+\"_eval.bytes\", 'wb') as file:\n",
    "    file.write(mini_batch_size.to_bytes(4, byteorder ='little',signed=True)) # number of minibatchs\n",
    "    file.write(len(target).to_bytes(4, byteorder ='little',signed=True)) # number of training examples\n",
    "    file.write((3).to_bytes(4, byteorder ='little',signed=True)) # number of tensors (contexts, labels, target)\n",
    "    file.write((len(target) * size_of_int).to_bytes(4, byteorder ='little',signed=True)) # size in bytes of target\n",
    "    for t in target:\n",
    "        file.write(int(t).to_bytes(4, byteorder ='little',signed=True))\n",
    "    file.write((len(contexts) * 5 * size_of_int).to_bytes(4, byteorder ='little',signed=True)) # size in bytes of context\n",
    "    for context in contexts:\n",
    "        for c in context:\n",
    "            file.write(int(c).to_bytes(4, byteorder ='little',signed=True))\n",
    "    file.write((len(labels) * 5 * size_of_int).to_bytes(4, byteorder ='little',signed=True)) # size in bytes of labels\n",
    "    for label in labels:\n",
    "        for l in label:\n",
    "            file.write(int(l).to_bytes(4, byteorder ='little',signed=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19886b-fe7d-443c-9f7b-af88d84a61b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
